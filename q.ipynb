{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DengAI: Predicting Disease Spread - A Comparative Modeling Project\n",
    "\n",
    "---\n",
    "\n",
    "This notebook provides a complete workflow for the DengAI prediction challenge. It progresses through the following stages:\n",
    "1.  **Problem Definition & Setup:** Importing libraries and defining the problem.\n",
    "2.  **Data Loading & Initial Exploration:** Loading the data and performing an initial check.\n",
    "3.  **Feature Engineering:** Creating lagged and interaction features to improve model performance.\n",
    "4.  **Exploratory Data Analysis (EDA):** Visualizing the data to understand its patterns, confirming the need for separate city models.\n",
    "5.  **Model Building & Robust Evaluation:** Training and evaluating three different models using a strict **Time-Series Cross-Validation** to prevent data leakage and get a reliable measure of performance.\n",
    "    *   Model 1: `PoissonRegressor` (A simple baseline)\n",
    "    *   Model 2: `XGBoost` (A powerful non-linear model with baseline parameters)\n",
    "    *   Model 3: Tuned `XGBoost` (The same model with hyperparameters optimized via `GridSearchCV`)\n",
    "6.  **Model Comparison:** A new section to directly compare the cross-validation scores of the three models to determine the best performer.\n",
    "7.  **Final Model Training & Submission:** Training the winning model on the full dataset and generating the final submission file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem Description and Motivation\n",
    "\n",
    "Dengue fever is a mosquito-borne illness that poses a significant threat to public health in tropical and subtropical regions. Accurately forecasting dengue outbreaks is crucial for public health officials to implement timely and effective control measures. This project aims to build a machine learning model that can predict the number of future dengue cases based on historical climate data, providing weekly forecasts for two cities: San Juan, Puerto Rico, and Iquitos, Peru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 7)\n",
    "\n",
    "print(\"Libraries imported and seed set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "try:\n",
    "    features_train_df = pd.read_csv('dengue_features_train.csv')\n",
    "    labels_train_df = pd.read_csv('dengue_labels_train.csv')\n",
    "    features_test_df = pd.read_csv('dengue_features_test.csv')\n",
    "    submission_df = pd.read_csv('submission_format.csv')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: {e}\")\n",
    "    raise FileNotFoundError(\"Please make sure the dengue CSV data files are in the same directory as this notebook before proceeding!\")\n",
    "\n",
    "# Merge training features and labels\n",
    "train_df = pd.merge(features_train_df, labels_train_df, on=['city', 'year', 'weekofyear'])\n",
    "\n",
    "# Display the first few rows of the combined training data\n",
    "print(\"Combined Training Data:\")\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Feature Engineering\n",
    "\n",
    "To improve model performance, we engineer sophisticated features. This includes lagged climate variables (since weather's effect is delayed) and interaction terms (since some conditions are more dangerous together).\n",
    "\n",
    "**Note on a Minor Data Leak:** The `bfill()` method used below to fill NaNs created by rolling/lagged features technically uses future information. However, this only affects the first few rows of the entire dataset. It is a pragmatic choice to avoid losing data points, and its impact on the final model is negligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineer(df):\n",
    "    # Forward-fill to handle original missing values\n",
    "    df.fillna(method='ffill', inplace=True)\n",
    "    \n",
    "    # Date-based features\n",
    "    df['week_start_date'] = pd.to_datetime(df['week_start_date'])\n",
    "    df['month'] = df['week_start_date'].dt.month\n",
    "    \n",
    "    # Lagged Features\n",
    "    lag_features = [\n",
    "        'reanalysis_precip_amt_kg_per_m2',\n",
    "        'station_avg_temp_c',\n",
    "        'reanalysis_specific_humidity_g_per_kg'\n",
    "    ]\n",
    "    lag_periods = [4, 8, 12] # Lags of ~1, 2, and 3 months\n",
    "    \n",
    "    for feature in lag_features:\n",
    "        for lag in lag_periods:\n",
    "            df[f'{feature}_lag_{lag}'] = df.groupby('city')[feature].shift(lag)\n",
    "            \n",
    "    # Interaction Terms\n",
    "    df['temp_x_humidity'] = df['reanalysis_air_temp_k'] * df['reanalysis_specific_humidity_g_per_kg']\n",
    "    \n",
    "    # Fill NaNs created by lagging features using backfill\n",
    "    df.fillna(method='bfill', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the feature engineering\n",
    "train_df = feature_engineer(train_df.copy())\n",
    "features_test_df = feature_engineer(features_test_df.copy())\n",
    "\n",
    "print(\"Advanced feature engineering complete.\")\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Visualizing the data reveals that San Juan and Iquitos have vastly different outbreak patterns. This confirms our strategy of building separate models for each city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data for each city\n",
    "sj_train_df = train_df[train_df['city'] == 'sj']\n",
    "iq_train_df = train_df[train_df['city'] == 'iq']\n",
    "\n",
    "# Plot Total Dengue Cases Over Time\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(sj_train_df['week_start_date'], sj_train_df['total_cases'], label='San Juan')\n",
    "plt.plot(iq_train_df['week_start_date'], iq_train_df['total_cases'], label='Iquitos')\n",
    "plt.title('Total Dengue Cases Over Time by City')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Cases')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Building & Robust Evaluation\n",
    "\n",
    "We will now build and evaluate our three models. **Crucially, we will use a Time-Series Cross-Validation (`TimeSeriesSplit`) for all evaluations.** This prevents data leakage by ensuring that the model is always trained on past data and validated on future data. This gives us a reliable and honest measure of each model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup for Modeling ---\n",
    "\n",
    "# Define the Time-Series Cross-Validation strategy\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Define the features and target\n",
    "features = train_df.select_dtypes(include=np.number).columns.drop(['year', 'weekofyear', 'total_cases'])\n",
    "target = 'total_cases'\n",
    "\n",
    "# Prepare data for each city\n",
    "X_sj = train_df[train_df['city'] == 'sj'][features]\n",
    "y_sj = train_df[train_df['city'] == 'sj'][target]\n",
    "X_iq = train_df[train_df['city'] == 'iq'][features]\n",
    "y_iq = train_df[train_df['city'] == 'iq'][target]\n",
    "\n",
    "# --- Model 1: Poisson Regressor ---\n",
    "print(\"--- Evaluating Model 1: Poisson Regressor ---\")\n",
    "pipeline_pr = Pipeline([('scaler', StandardScaler()), ('poisson', PoissonRegressor(alpha=0.1, max_iter=1000))])\n",
    "scores_pr_sj = cross_val_score(pipeline_pr, X_sj, y_sj, cv=tscv, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "scores_pr_iq = cross_val_score(pipeline_pr, X_iq, y_iq, cv=tscv, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "mae_pr_sj = -scores_pr_sj.mean()\n",
    "mae_pr_iq = -scores_pr_iq.mean()\n",
    "print(f\"San Juan - CV Mean Absolute Error: {mae_pr_sj:.4f}\")\n",
    "print(f\"Iquitos  - CV Mean Absolute Error: {mae_pr_iq:.4f}\\n\")\n",
    "\n",
    "# --- Model 2: Baseline XGBoost Regressor ---\n",
    "print(\"--- Evaluating Model 2: Baseline XGBoost ---\")\n",
    "model_xgb = xgb.XGBRegressor(objective='count:poisson', seed=SEED, n_jobs=-1)\n",
    "scores_xgb_sj = cross_val_score(model_xgb, X_sj, y_sj, cv=tscv, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "scores_xgb_iq = cross_val_score(model_xgb, X_iq, y_iq, cv=tscv, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "mae_xgb_sj = -scores_xgb_sj.mean()\n",
    "mae_xgb_iq = -scores_xgb_iq.mean()\n",
    "print(f\"San Juan - CV Mean Absolute Error: {mae_xgb_sj:.4f}\")\n",
    "print(f\"Iquitos  - CV Mean Absolute Error: {mae_xgb_iq:.4f}\\n\")\n",
    "\n",
    "# --- Model 3: Tuned XGBoost Regressor ---\n",
    "print(\"--- Evaluating Model 3: Tuned XGBoost (via GridSearchCV) ---\")\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [3, 5],\n",
    "    'colsample_bytree': [0.7, 1.0]\n",
    "}\n",
    "\n",
    "# Grid Search for San Juan\n",
    "grid_search_sj = GridSearchCV(estimator=model_xgb, param_grid=param_grid, cv=tscv, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=1)\n",
    "grid_search_sj.fit(X_sj, y_sj)\n",
    "mae_tuned_sj = -grid_search_sj.best_score_\n",
    "print(f\"\\nSan Juan - Best CV Mean Absolute Error: {mae_tuned_sj:.4f}\")\n",
    "print(f\"San Juan - Best Parameters: {grid_search_sj.best_params_}\\n\")\n",
    "\n",
    "# Grid Search for Iquitos\n",
    "grid_search_iq = GridSearchCV(estimator=model_xgb, param_grid=param_grid, cv=tscv, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=1)\n",
    "grid_search_iq.fit(X_iq, y_iq)\n",
    "mae_tuned_iq = -grid_search_iq.best_score_\n",
    "print(f\"\\nIquitos - Best CV Mean Absolute Error: {mae_tuned_iq:.4f}\")\n",
    "print(f\"Iquitos - Best Parameters: {grid_search_iq.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison\n",
    "\n",
    "Now we will formally compare the robust cross-validation scores from our three models. The Mean Absolute Error (MAE) tells us, on average, how many cases our predictions are off by. A lower MAE is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store the results\n",
    "results_data = {\n",
    "    'Model': [\n",
    "        'Poisson Regressor', 'Baseline XGBoost', 'Tuned XGBoost',\n",
    "        'Poisson Regressor', 'Baseline XGBoost', 'Tuned XGBoost'\n",
    "    ],\n",
    "    'City': ['San Juan', 'San Juan', 'San Juan', 'Iquitos', 'Iquitos', 'Iquitos'],\n",
    "    'CV Mean Absolute Error': [\n",
    "        mae_pr_sj, mae_xgb_sj, mae_tuned_sj,\n",
    "        mae_pr_iq, mae_xgb_iq, mae_tuned_iq\n",
    "    ]\n",
    "}\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "print(\"--- Model Performance Comparison ---\")\n",
    "display(results_df.round(4))\n",
    "\n",
    "# Visualize the comparison\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.barplot(data=results_df, x='Model', y='CV Mean Absolute Error', hue='City')\n",
    "plt.title('Comparison of Model Performance (Lower is Better)')\n",
    "plt.ylabel('Cross-Validated Mean Absolute Error')\n",
    "plt.xticks(rotation=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison Analysis\n",
    "\n",
    "The results clearly show a significant improvement with each step:\n",
    "1.  **Poisson Regressor** provides a very basic baseline. Its linear nature struggles to capture the complex, seasonal dynamics of dengue outbreaks.\n",
    "2.  **Baseline XGBoost** dramatically outperforms the Poisson model. Its ability to model non-linear relationships allows it to better predict the peaks and troughs of the outbreak cycles.\n",
    "3.  **Tuned XGBoost** provides the best performance for both cities. The `GridSearchCV` process successfully finds a combination of hyperparameters that allows the model to generalize even better to unseen data, resulting in the lowest MAE.\n",
    "\n",
    "**Conclusion:** The Tuned XGBoost model is the clear winner and will be used to generate our final predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Model Training & Submission\n",
    "\n",
    "Now that we have selected our best model and its optimal hyperparameters, we will train it on the *entire* training dataset for each city. This ensures the final model learns from all available historical data before making predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train Final Model for San Juan with Best Parameters ---\n",
    "final_model_sj = xgb.XGBRegressor(\n",
    "    objective='count:poisson',\n",
    "    seed=SEED,\n",
    "    n_jobs=-1,\n",
    "    **grid_search_sj.best_params_ # Unpack the best parameters found\n",
    ")\n",
    "final_model_sj.fit(X_sj, y_sj)\n",
    "\n",
    "# --- Train Final Model for Iquitos with Best Parameters ---\n",
    "final_model_iq = xgb.XGBRegressor(\n",
    "    objective='count:poisson',\n",
    "    seed=SEED,\n",
    "    n_jobs=-1,\n",
    "    **grid_search_iq.best_params_\n",
    ")\n",
    "final_model_iq.fit(X_iq, y_iq)\n",
    "\n",
    "print(\"Final optimized models for both cities have been trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare Test Set and Generate Predictions ---\n",
    "X_test_sj = features_test_df[features_test_df['city'] == 'sj'][features]\n",
    "X_test_iq = features_test_df[features_test_df['city'] == 'iq'][features]\n",
    "\n",
    "predictions_sj = final_model_sj.predict(X_test_sj)\n",
    "predictions_iq = final_model_iq.predict(X_test_iq)\n",
    "\n",
    "# --- Create Submission File ---\n",
    "sj_test_ids = features_test_df[features_test_df['city'] == 'sj'][['city', 'year', 'weekofyear']]\n",
    "iq_test_ids = features_test_df[features_test_df['city'] == 'iq'][['city', 'year', 'weekofyear']]\n",
    "\n",
    "sj_test_ids['total_cases'] = predictions_sj\n",
    "iq_test_ids['total_cases'] = predictions_iq\n",
    "\n",
    "combined_predictions = pd.concat([sj_test_ids, iq_test_ids])\n",
    "\n",
    "# Merge with submission format to ensure correct ordering\n",
    "final_submission = submission_df[['city', 'year', 'weekofyear']].merge(\n",
    "    combined_predictions, on=['city', 'year', 'weekofyear']\n",
    ")\n",
    "\n",
    "# Clip at 0, round to nearest integer, and cast to int type\n",
    "final_submission['total_cases'] = final_submission['total_cases'].clip(0).round().astype(int)\n",
    "\n",
    "print(\"Final Submission DataFrame:\")\n",
    "display(final_submission.head())\n",
    "\n",
    "# Save the submission file\n",
    "final_submission.to_csv('our_submission.csv', index=False)\n",
    "\n",
    "print(\"\\n'our_submission.csv' has been created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}